# ğŸ“ Directory Structure

### `pretrained/`
Contains models saved directly from `training.ipynb`.

### `fineTuned/`
Contains models saved after further training in `finetuning.ipynb`.

---

# ğŸ“ Naming Conventions

- `EnYo` â†’ English to Yoruba  
- `YoEn` â†’ Yoruba to English  
- `FineTuned` â†’ Prefix for models saved during or after finetuning  
- `Base` â†’ Suffix for models saved at final epoch or via early stopping  
- `Best` â†’ Suffix for models saved by `ModelCheckpoint` based on `val_masked_accuracy`

> **Note:**  
> All saved models **exclude the optimizerâ€™s state** to reduce file size. This keeps each model under GitHubâ€™s 100â€¯MB file limit (each is ~76â€¯MB).

---

# ğŸš€ Model Usage

To translate a tokenized input:
```python
translated_tokens = model.translate(tokens)
```

- **`tokens`** should be a tensor containing tokenized input.
- The function returns **translated tokens**, which can then be decoded into a readable sequence.
- **Translation method:** Greedy Search (selects the most probable token at each step).

---

## âš ï¸ Observed Limitations

- **EnYo** (English â†’ Yoruba) models perform consistently well on both **CPU** and **GPU**.
- **YoEn** (Yoruba â†’ English) models perform best when run **in batches on GPU**, similar to the training condition.
- **Translation quality may degrade** when using `YoEn` models on **CPU** or for **single-sample inference**.

---

## ğŸ“¦ Model Size

Each model uploaded to this repository is approximately **76â€¯MB** in size.  
This compact size is achieved by **excluding the optimizer's state** from the saved model files.

